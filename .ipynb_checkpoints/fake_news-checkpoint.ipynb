{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fake news challenge\n",
    "\n",
    "In this task, we will have to build a ML model that gives a confidence score between 1 and 0 to an article (is this article reliable or not). The requirements are:\n",
    "\n",
    "- train a model on the Kaggle fake news dataset https://www.kaggle.com/c/fake-news/data .\n",
    "- provide the metrics to assess our model on the testing set (accuracy etc)\n",
    "- build a final function give_score(link) that takes a link of an article as an input and outputs the confidence score. \n",
    "- compare 3 different ML technics.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "\n",
    "**Required libraries to run this notebook:**\n",
    "\n",
    "*Included in Anaconda distribution:*\n",
    "- numpy\n",
    "- pandas\n",
    "- scikit learn\n",
    "- requests\n",
    "- bs4\n",
    "\n",
    "*Not included in Anaconda distribution:*\n",
    "- [newspaper](https://github.com/codelucas/newspaper) *( $\\rightarrow$ pip3 install newspaper3k)*\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### **1) Importing useful libraries:**\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-22T18:46:00.016915Z",
     "start_time": "2018-10-22T18:46:00.010116Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### **2) Loading phase:**\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "FN_DATA_FOLDER = 'fake_news_data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv(FN_DATA_FOLDER+\"/train.csv\")\n",
    "test_data = pd.read_csv(FN_DATA_FOLDER+\"/test.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data we will use in this notebook is a set of press articles. On each of them, we have the following information:\n",
    "\n",
    "- **`id`**: unique id for a news article\n",
    "- **`title`**: the title of a news article\n",
    "- **`author`**: author of the news article\n",
    "- **`text`**: the text of the article; could be incomplete\n",
    "- **`label`**: a label that marks the article as potentially unreliable\n",
    "    - `1`: unreliable\n",
    "    - `0`: reliable\n",
    "    \n",
    "Let's take a closer look at our dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data :\n",
      "--------------------------------------------------\n",
      "Total number of articles:  20800\n",
      "Number of fake news:  10413\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>author</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>House Dem Aide: We Didn’t Even See Comey’s Let...</td>\n",
       "      <td>Darrell Lucus</td>\n",
       "      <td>House Dem Aide: We Didn’t Even See Comey’s Let...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>FLYNN: Hillary Clinton, Big Woman on Campus - ...</td>\n",
       "      <td>Daniel J. Flynn</td>\n",
       "      <td>Ever get the feeling your life circles the rou...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Why the Truth Might Get You Fired</td>\n",
       "      <td>Consortiumnews.com</td>\n",
       "      <td>Why the Truth Might Get You Fired October 29, ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                                              title              author  \\\n",
       "0   0  House Dem Aide: We Didn’t Even See Comey’s Let...       Darrell Lucus   \n",
       "1   1  FLYNN: Hillary Clinton, Big Woman on Campus - ...     Daniel J. Flynn   \n",
       "2   2                  Why the Truth Might Get You Fired  Consortiumnews.com   \n",
       "\n",
       "                                                text  label  \n",
       "0  House Dem Aide: We Didn’t Even See Comey’s Let...      1  \n",
       "1  Ever get the feeling your life circles the rou...      0  \n",
       "2  Why the Truth Might Get You Fired October 29, ...      1  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Train data :\")\n",
    "print('--------------------------------------------------')\n",
    "print('Total number of articles: ',len(train_data))\n",
    "print('Number of fake news: ',len(train_data[train_data.label==1]))\n",
    "train_data.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test data :\n",
      "--------------------------------------------------\n",
      "Number of articles:  5200\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>author</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20800</td>\n",
       "      <td>Specter of Trump Loosens Tongues, if Not Purse...</td>\n",
       "      <td>David Streitfeld</td>\n",
       "      <td>PALO ALTO, Calif.  —   After years of scorning...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20801</td>\n",
       "      <td>Russian warships ready to strike terrorists ne...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Russian warships ready to strike terrorists ne...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20802</td>\n",
       "      <td>#NoDAPL: Native American Leaders Vow to Stay A...</td>\n",
       "      <td>Common Dreams</td>\n",
       "      <td>Videos #NoDAPL: Native American Leaders Vow to...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      id                                              title            author  \\\n",
       "0  20800  Specter of Trump Loosens Tongues, if Not Purse...  David Streitfeld   \n",
       "1  20801  Russian warships ready to strike terrorists ne...               NaN   \n",
       "2  20802  #NoDAPL: Native American Leaders Vow to Stay A...     Common Dreams   \n",
       "\n",
       "                                                text  \n",
       "0  PALO ALTO, Calif.  —   After years of scorning...  \n",
       "1  Russian warships ready to strike terrorists ne...  \n",
       "2  Videos #NoDAPL: Native American Leaders Vow to...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Test data :\")\n",
    "print('--------------------------------------------------')\n",
    "print('Number of articles: ',len(test_data))\n",
    "test_data.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### **3) Cleaning phase:**\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see there are some empty cell in our csv files, resulting in **`Nan`** values in our dataset. \n",
    "\n",
    "Let's start by seeing how many they are:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "Number of NaN values in x_train by column:\n",
      "\t [0, 558, 1957, 39, 0]\n",
      "--------------------------------------------------\n",
      "Number of NaN values in x_test by column:\n",
      "\t [0, 122, 503, 7]\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "print('--------------------------------------------------')\n",
    "print('Number of NaN values in x_train by column:')\n",
    "print('\\t',train_data.isna().sum().tolist())\n",
    "print('--------------------------------------------------')\n",
    "print('Number of NaN values in x_test by column:')\n",
    "print('\\t',test_data.isna().sum().tolist())\n",
    "print('--------------------------------------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "To correct these values, we simply replace the empty cells by a space to mark them as empty.\n",
    "\n",
    "> We can see that for some articles, the entire text of the article is missing. Very little information is therefore available, but this data can still be interesting. We can indeed have information on the reliability of an author for example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = train_data.fillna(' ')\n",
    "test_data = test_data.fillna(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "Number of NaN values in x_train by column:\n",
      "\t [0, 0, 0, 0, 0]\n",
      "--------------------------------------------------\n",
      "Number of NaN values in x_test by column:\n",
      "\t [0, 0, 0, 0]\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "print('--------------------------------------------------')\n",
    "print('Number of NaN values in x_train by column:')\n",
    "print('\\t',train_data.isna().sum().tolist())\n",
    "print('--------------------------------------------------')\n",
    "print('Number of NaN values in x_test by column:')\n",
    "print('\\t',test_data.isna().sum().tolist())\n",
    "print('--------------------------------------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our dataset is cleaned !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### **4) Articles preprocessing:**\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "**Our objective is to classify articles based on their content using Scikit Learn.**\n",
    "\n",
    "We have two dataset with texts. Unfortunaltly, algorithms are not really good with numbers, so we'll need to find a way to **convert our texts to numbers without losing information**. To do that, we'll see that there are many ways, some of which are more optimized than others."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer, TfidfVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "A first step of preprocession is to concatenate the `title`, `author` and `text` for each article inside one column that we will process in the next steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>author</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>full_article</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>House Dem Aide: We Didn’t Even See Comey’s Let...</td>\n",
       "      <td>Darrell Lucus</td>\n",
       "      <td>House Dem Aide: We Didn’t Even See Comey’s Let...</td>\n",
       "      <td>1</td>\n",
       "      <td>House Dem Aide: We Didn’t Even See Comey’s Let...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>FLYNN: Hillary Clinton, Big Woman on Campus - ...</td>\n",
       "      <td>Daniel J. Flynn</td>\n",
       "      <td>Ever get the feeling your life circles the rou...</td>\n",
       "      <td>0</td>\n",
       "      <td>FLYNN: Hillary Clinton, Big Woman on Campus - ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Why the Truth Might Get You Fired</td>\n",
       "      <td>Consortiumnews.com</td>\n",
       "      <td>Why the Truth Might Get You Fired October 29, ...</td>\n",
       "      <td>1</td>\n",
       "      <td>Why the Truth Might Get You Fired Consortiumne...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                                              title              author  \\\n",
       "0   0  House Dem Aide: We Didn’t Even See Comey’s Let...       Darrell Lucus   \n",
       "1   1  FLYNN: Hillary Clinton, Big Woman on Campus - ...     Daniel J. Flynn   \n",
       "2   2                  Why the Truth Might Get You Fired  Consortiumnews.com   \n",
       "\n",
       "                                                text  label  \\\n",
       "0  House Dem Aide: We Didn’t Even See Comey’s Let...      1   \n",
       "1  Ever get the feeling your life circles the rou...      0   \n",
       "2  Why the Truth Might Get You Fired October 29, ...      1   \n",
       "\n",
       "                                        full_article  \n",
       "0  House Dem Aide: We Didn’t Even See Comey’s Let...  \n",
       "1  FLYNN: Hillary Clinton, Big Woman on Campus - ...  \n",
       "2  Why the Truth Might Get You Fired Consortiumne...  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data['full_article']=train_data['title']+' '+train_data['author']+' '+train_data['text']\n",
    "test_data['full_article']=test_data['title']+' '+test_data['author']+' '+test_data['text']\n",
    "\n",
    "train_data.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "####  **count_vectorizer()**\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vectorizer = CountVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = count_vectorizer.fit_transform(train_data['full_article'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">`count_vectorizer.fit_transform()` allows us to extract every unique features (words) in our texts and convert them to a matrix of token counts $\\rightarrow$ `1`if the word is present in the article, `0` otherwise.\n",
    "\n",
    "Let's take a look at our resulting data in more details:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--------------------------------------------------\n",
      "Number of different features in all articles:\n",
      "\t 182061\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "print('\\n--------------------------------------------------')\n",
    "print('Number of different features in all articles:')\n",
    "print('\\t',x_train.shape[1])\n",
    "print('--------------------------------------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The number of features is actually the number of unique words present in our article: **`182.061`**\n",
    "\n",
    "This number is obviously huge, but most importantly, **it is not optimized**. Indeed, we know that words in a language are not evenly distributed across a corpus; instead, there are a few words that are very common, and a very large number of words that are rare: They follow a [zipf distribution](https://www.youtube.com/watch?v=fCn8zs912OE).\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **tfidf_vectorizer()**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">`tfidf` = short for **term frequency–inverse document frequency**\n",
    "\n",
    "The idea behind the `tfidf_vectorizer` is to reflect how important a word is to a document, so in fact it will allow us to extract **key words** from a document.\n",
    "\n",
    "<br>\n",
    "\n",
    "To do so, it basically uses the **frequency** of appearance of a word in a text $\\rightarrow$ **`tf`**. Indeed, we coult think that the most used words are the most important and relevant for our article. Unfortunatly, as we said earlier, most of the words used are very common words, not specific to our article at all!\n",
    "\n",
    "So what we really want, are words that appear a lot **in one article**, BUT **not a lot in a corpus of texts**, in other words in documents $\\rightarrow$ **`idf`**.\n",
    "\n",
    "<br>\n",
    "\n",
    "Let's take a look at `tfidf_vectorizer` in action, using sentences that will correspond to our articles:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles = np.array(['the Future of Mobile',\n",
    "        'Soon on IOS & Android',\n",
    "        'Gen Z and millennials',\n",
    "        'everaging ML and DL',\n",
    "        'personalized, high-quality'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "Let's begin by using `TfidfVectorizer` with all parameters set to default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_1 = TfidfVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matrix of words occurencies in our articles:\n",
      "\n",
      "[[0.  0.  0.  0.  0.5 0.  0.  0.  0.  0.  0.5 0.5 0.  0.  0.  0.  0.5]\n",
      " [0.  0.5 0.  0.  0.  0.  0.  0.5 0.  0.  0.  0.  0.5 0.  0.  0.5 0. ]\n",
      " [0.5 0.  0.  0.  0.  0.6 0.  0.  0.6 0.  0.  0.  0.  0.  0.  0.  0. ]\n",
      " [0.4 0.  0.5 0.5 0.  0.  0.  0.  0.  0.5 0.  0.  0.  0.  0.  0.  0. ]\n",
      " [0.  0.  0.  0.  0.  0.  0.6 0.  0.  0.  0.  0.  0.  0.6 0.6 0.  0. ]]\n"
     ]
    }
   ],
   "source": [
    "print('Matrix of words occurencies in our articles:\\n')\n",
    "matrix = tfidf_1.fit_transform(articles).toarray()\n",
    "print(np.vectorize(lambda x:round(x,1))(matrix))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, we managed to get a **matrix of words occurencies** in our articles. Moreover, each word present in an article is given a **score** which represents how relevant it is.\n",
    "\n",
    "*(The score above is rounded for readability)*\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keywords in our articles:\n",
      "--------------------------\n",
      "future - mobile - of - the\n",
      "android - ios - on - soon\n",
      "and - gen - millennials\n",
      "and - dl - everaging - ml\n",
      "high - personalized - quality\n"
     ]
    }
   ],
   "source": [
    "print('Keywords in our articles:\\n--------------------------')\n",
    "keywords = tfidf_1.inverse_transform(matrix)\n",
    "for array in keywords:\n",
    "    print(' - '.join(keyword for keyword in array))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "As we can see above, words are scored to show their importance, but we still have all the most common words which are not specific to our article at all.\n",
    "\n",
    "These words are called **stop words**. In English for example, they consist of words like `the`, `and` etc. \n",
    "\n",
    "We can simply add a parameter to our tfidf to automatically remove them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_2 = TfidfVectorizer(stop_words='english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keywords in our articles:\n",
      "--------------------------\n",
      "future - mobile\n",
      "android - ios - soon\n",
      "gen - millennials\n",
      "dl - everaging - ml\n",
      "high - personalized - quality\n"
     ]
    }
   ],
   "source": [
    "print('Keywords in our articles:\\n--------------------------')\n",
    "matrix = tfidf_2.fit_transform(articles).toarray()\n",
    "matrix = np.vectorize(lambda x:round(x,2))(matrix)\n",
    "keywords = tfidf_2.inverse_transform(matrix)\n",
    "for array in keywords:\n",
    "    print(' - '.join(keyword for keyword in array))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "As we can see, by removing stop words, **only key words are extracted** from our (very short) articles.\n",
    "\n",
    "<br>\n",
    "\n",
    "We can now use `tfidf.fit_transform()` to convert them to a matrix of token counts, just as we did with `count_vectorizer`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>android</th>\n",
       "      <th>dl</th>\n",
       "      <th>everaging</th>\n",
       "      <th>future</th>\n",
       "      <th>gen</th>\n",
       "      <th>high</th>\n",
       "      <th>ios</th>\n",
       "      <th>millennials</th>\n",
       "      <th>ml</th>\n",
       "      <th>mobile</th>\n",
       "      <th>personalized</th>\n",
       "      <th>quality</th>\n",
       "      <th>soon</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>the Future of Mobile</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.71</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.71</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Soon on IOS &amp; Android</th>\n",
       "      <td>0.58</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.58</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gen Z and millennials</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.71</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.71</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>everaging ML and DL</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.58</td>\n",
       "      <td>0.58</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.58</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>personalized, high-quality</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.58</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.58</td>\n",
       "      <td>0.58</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            android    dl  everaging  future   gen  high  \\\n",
       "the Future of Mobile           0.00  0.00       0.00    0.71  0.00  0.00   \n",
       "Soon on IOS & Android          0.58  0.00       0.00    0.00  0.00  0.00   \n",
       "Gen Z and millennials          0.00  0.00       0.00    0.00  0.71  0.00   \n",
       "everaging ML and DL            0.00  0.58       0.58    0.00  0.00  0.00   \n",
       "personalized, high-quality     0.00  0.00       0.00    0.00  0.00  0.58   \n",
       "\n",
       "                             ios  millennials    ml  mobile  personalized  \\\n",
       "the Future of Mobile        0.00         0.00  0.00    0.71          0.00   \n",
       "Soon on IOS & Android       0.58         0.00  0.00    0.00          0.00   \n",
       "Gen Z and millennials       0.00         0.71  0.00    0.00          0.00   \n",
       "everaging ML and DL         0.00         0.00  0.58    0.00          0.00   \n",
       "personalized, high-quality  0.00         0.00  0.00    0.00          0.58   \n",
       "\n",
       "                            quality  soon  \n",
       "the Future of Mobile           0.00  0.00  \n",
       "Soon on IOS & Android          0.00  0.58  \n",
       "Gen Z and millennials          0.00  0.00  \n",
       "everaging ML and DL            0.00  0.00  \n",
       "personalized, high-quality     0.58  0.00  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(matrix, index=articles, columns=tfidf_2.get_feature_names())\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "This is pretty good, we've managed to remove a lot of useless data from our articles without losing any information.\n",
    "\n",
    "\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "\n",
    "**Let's get back to our dataset and apply `tf-idf` transformation to our articles :**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer(stop_words='english', ngram_range=(1, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = tfidf_vectorizer.fit_transform(train_data['full_article'].values)\n",
    "x_test = tfidf_vectorizer.transform(test_data['full_article'].values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">Using `transform` instead of `fit_transform` preserves the vocabulary created from `fit_transform` in the previous line, and ensures identical columns for these matrices."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### **5) Learning phase:**\n",
    "<br>\n",
    "\n",
    "Now that we have extracted the features, we can train a classifier to try to predict the reliability of an article:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = train_data['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score #to get accuracies and compare results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "Model 1: **MultinomialNB**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Let’s start with a naïve Bayes classifier, which provides a nice baseline for this task. scikit-learn includes several variants of this classifier; the one most suitable for word counts is the multinomial variant:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy:  0.8773560756901098\n",
      "std:  0.006281584150964021\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "model1 = MultinomialNB()\n",
    "model1.fit(x_train, y_train)\n",
    "\n",
    "accuracies = cross_val_score(estimator=model1, X=x_train, y=y_train, cv=10)\n",
    "mean_acc = accuracies.mean()\n",
    "std_acc = accuracies.std()\n",
    "print('accuracy: ',mean_acc)\n",
    "print('std: ',std_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "Model 2: **SGDClassifier**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s see if we can do better with a linear support vector machine (SVM), which is considered as one of the best text classification algorithms (although it’s also a bit slower than naïve Bayes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy:  0.9751917494691401\n",
      "std:  0.0030616149951694857\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "model2 = SGDClassifier(loss=\"hinge\", penalty=\"l2\", max_iter=15)\n",
    "model2.fit(x_train, y_train)\n",
    "\n",
    "accuracies = cross_val_score(estimator=model2, X=x_train, y=y_train, cv=10)\n",
    "mean_acc = accuracies.mean()\n",
    "std_acc = accuracies.std()\n",
    "print('accuracy: ',mean_acc)\n",
    "print('std: ',std_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "Model 3: **LogisticRegression**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy:  0.9750957111592022\n",
      "std:  0.002805510410011197\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "model3 = LogisticRegression(C=1e5)\n",
    "model3.fit(x_train, y_train)\n",
    "\n",
    "accuracies = cross_val_score(estimator=model2, X=x_train, y=y_train, cv=10)\n",
    "mean_acc = accuracies.mean()\n",
    "std_acc = accuracies.std()\n",
    "print('accuracy: ',mean_acc)\n",
    "print('std: ',std_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### **6) Making predictions:**\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that our model has been trained, we can make predictions using the features from the test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 1, ..., 0, 1, 0])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = model2.predict(x_test)\n",
    "\n",
    "#Display our predictions - they are either 0 or 1 for each training instance \n",
    "#depending on whether our algorithm believes the article is fake or not.\n",
    "predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "Since the data comes from a kaggle competiton, we will make a submission to get our score and see the performance of our model.\n",
    "\n",
    "To do that, we create a dataFrame with articles ids and our prediction regarding whether they are fake or not:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20800</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20801</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20802</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>20803</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20804</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      id  label\n",
       "0  20800      0\n",
       "1  20801      1\n",
       "2  20802      1\n",
       "3  20803      0\n",
       "4  20804      1"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission = pd.DataFrame({'id':test_data['id'],'label':predictions})\n",
    "\n",
    "submission.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--------------------------------------\n",
      "Saved file: articles_prediction_4.csv\n",
      "--------------------------------------\n"
     ]
    }
   ],
   "source": [
    "#Convert DataFrame to a csv file that can be uploaded\n",
    "filename = 'articles_prediction_4.csv'\n",
    "\n",
    "submission.to_csv(filename,index=False)\n",
    "print('\\n--------------------------------------')\n",
    "print('Saved file: ' + filename)\n",
    "print('--------------------------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "**Kaggle score : `0.98241`**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### **7) Testing the reliability of a given article:**\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this last part, we will try to use our model to predict the reliability af an article found on the web, giving it a score:\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by choosing a web article:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter a link to an article: https://www.nytimes.com/2018/11/09/us/politics/matthew-whitaker-donald-trump.html?action=click&module=Top%20Stories&pgtype=Homepage\n"
     ]
    }
   ],
   "source": [
    "url = input('Enter a link to an article: ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "\n",
    "**First approach:** using [`BeautifulSoup`](https://pypi.org/project/beautifulsoup4/) library:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "First thing we need to do, is to read the HTML for this article. We dot that using the `requests` library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------\n",
      "<!DOCTYPE html>\n",
      "<html lang=\"en\" itemId=\"https://www.nytimes.com/2018/11/09/us/politics/matthew-whitaker-donald-trump.html\" itemType=\"http://schema.org/NewsArticle\" itemScope=\"true\" class=\"story\" xmlns:og=\"http://opengraphprotocol.org/schema/\">\n",
      "  <head>\n",
      "    <title data-rh=\"true\">Trump Says ‘I Don’t K\n",
      "---------------------------------\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "article = requests.get(url)\n",
    "print('---------------------------------')\n",
    "print(article.text[0:300])\n",
    "print('---------------------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the HTML content of the web article, we need to extract useful informations from it.\n",
    "\n",
    "We will do that by using `BeautifulSoup`, which is a popular python library for web scrapping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "soup = BeautifulSoup(article.text, 'html.parser')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Getting all page classes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = [value\n",
    "           for element in soup.find_all(class_=True)\n",
    "           for value in element[\"class\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "classes containing \"author\":\n",
      "--------------------------\n"
     ]
    }
   ],
   "source": [
    "print('classes containing \"author\":\\n--------------------------')\n",
    "for class_name in classes:\n",
    "    if 'author' in class_name:\n",
    "        print(class_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**`title`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "title = soup.find('title')\n",
    "#print(title.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`Author`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "author = soup.find(attrs={'class': 'author'})\n",
    "#print(author.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "As we can understand from the code above, this approach works, but will recquire a lot of work to be fully generic and be able to parse any article on the web.\n",
    "\n",
    "Indeed, html tags can vary a lot from one site to another, so it might be difficul to think about all possible implementations..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "\n",
    "**Second approach:** using [`newspaper`](https://github.com/codelucas/newspaper) library:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Author :\n",
      "--------------------------\n",
      " ['Eileen Sullivan'] \n",
      "\n",
      "Title :\n",
      "--------------------------\n",
      " Trump Says ‘I Don’t Know Matt Whitaker,’ the Acting Attorney General He Chose \n",
      "\n",
      "Text :\n",
      "--------------------------\n",
      " WASHINGTON — President Trump said on Friday that he has not yet spoken to the new acting attorney general, Matthew G. Whitaker, about the special counsel investigation, and he distanced himself from Mr. Whitaker by suggesting that he did not know him.\n",
      "\n",
      "Mr. Whitaker, who now oversees the investigatio \n",
      "\n"
     ]
    }
   ],
   "source": [
    "from newspaper import Article\n",
    "\n",
    "try:\n",
    "    article = Article(url)\n",
    "    article.download()\n",
    "    article.parse()\n",
    "    title=article.title\n",
    "    authors=article.authors\n",
    "    text = article.text\n",
    "except:\n",
    "    print(\"Problem downloading the user article\")\n",
    "        \n",
    "print('Author :\\n--------------------------\\n',authors,'\\n')\n",
    "print('Title :\\n--------------------------\\n',title,'\\n')\n",
    "print('Text :\\n--------------------------\\n',text[0:300],'\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the `newspaper` library is pretty powerful and allows us to **parse an article really easily**! It allows us to directly separate distinct information from the article. \n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's apply our `tf-idf` transformation to the selected article:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_article = pd.DataFrame(columns=['full_article'])\n",
    "\n",
    "full_article=article.title\n",
    "for author in article.authors:\n",
    "    full_article+=' '+author\n",
    "full_article+=' '+article.text\n",
    "\n",
    "test_article.loc[0]= full_article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_article = tfidf_vectorizer.transform(test_article['full_article']).toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "Before making a prediction on the reliability ot the article, let's simply take a look a the keywords present in the article that were used:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array(['according', 'according people', 'acting', 'acting attorney',\n",
       "        'attorney', 'attorney general', 'chemistry', 'chose', 'counsel',\n",
       "        'counsel investigation', 'did', 'did know', 'distanced',\n",
       "        'distanced mr', 'don', 'don know', 'easy', 'eileen', 'familiar',\n",
       "        'friday', 'general', 'highly', 'highly respected', 'investigation',\n",
       "        'know', 'know mr', 'left', 'left washington', 'man', 'matt',\n",
       "        'matt whitaker', 'matthew', 'matthew whitaker', 'mr', 'mr trump',\n",
       "        'mr whitaker', 'new', 'new acting', 'office', 'office times',\n",
       "        'oval', 'oval office', 'oversees', 'paris', 'people',\n",
       "        'people familiar', 'president', 'president according',\n",
       "        'president trump', 'relationship', 'relationship don', 'reporters',\n",
       "        'reporters left', 'respected', 'respected man', 'said',\n",
       "        'said easy', 'said friday', 'says', 'says don', 'special',\n",
       "        'special counsel', 'spoken', 'spoken new', 'suggesting',\n",
       "        'suggesting did', 'sullivan', 'times', 'times said', 'told',\n",
       "        'told reporters', 'trip', 'trip paris', 'trump', 'trump said',\n",
       "        'trump says', 'trump told', 'visited', 'washington',\n",
       "        'washington president', 'washington weekend', 'weekend',\n",
       "        'weekend trip', 'whitaker'], dtype='<U129')]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_vectorizer.inverse_transform(x_article)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "Let's now see what our model thinks about it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Reliability :  99.23729999999999 %\n"
     ]
    }
   ],
   "source": [
    "prediction = model3.predict_proba(x_article)\n",
    "print('\\nReliability : ',round(prediction[0,0],6)*100,'%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "*Same code as above, regrouped inside a function that takes a link of an article as an input and outputs the confidence score*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def give_score(link):\n",
    "    # Parsing the article\n",
    "    article = Article(link)\n",
    "    article.download()\n",
    "    article.parse()\n",
    "    # Applying our tf-idf vectorizer\n",
    "    test_article = pd.DataFrame(columns=['full_article'])\n",
    "    full_article=article.title\n",
    "    for author in article.authors:\n",
    "        full_article+=' '+author\n",
    "    full_article+=' '+article.text\n",
    "    test_article.loc[0]= full_article\n",
    "    x_article = tfidf_vectorizer.transform(test_article['full_article']).toarray()\n",
    "    \n",
    "    return model3.predict_proba(x_article)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Reliability :  99.23729999999999 %\n"
     ]
    }
   ],
   "source": [
    "print('\\nReliability : ',round(give_score(url)[0,0],6)*100,'%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "<br>\n",
    "\n",
    "By making many tests of the model on articles found on the web, I realized that **the performance of our model is far from consistent**.\n",
    "\n",
    "Indeed, I started by testing it on articles on which it had not trained (obviously), but from **sites it knew**. In these cases, the performances were very (too?) good!\n",
    "\n",
    "For example, a random article from *The New York Time* gave a reliability of `95%`, when one from *Liberal America* only gave `4%`.\n",
    "\n",
    "On the other hand, when the articles came from **less common sources**, on which our model probably had never trained, the performances were not as good, or even completely wrong.\n",
    "\n",
    "<br>\n",
    "\n",
    "**Some ideas to improve our current model:**\n",
    "\n",
    "- Taking a lot more articles to train our model. Most of the time, increasing the size of the data used can be easier than tuning every parameter over and over.\n",
    "- Of course, better tuning the parameters of the model used could definitely help imporving our results a little bit.\n",
    "- Change the overly simplistic approach we have had. Indeed, in the end we only looked for keywords in the articles. However, it is quite clear that this will not be enough if we want an effective tool on a large number of articles, from a much wider spectrum of sources. For example, it might be useful to check the source and author of the article separately. At the same time, we could also check only the number of digits, percentages, dates and comparisons in the article.\n",
    "\n",
    "\n",
    "<br>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
